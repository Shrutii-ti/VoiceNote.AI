# Voice-to-Notes AI Backend

A clean, well-structured Node.js + Express backend for a Voice-to-Notes AI application using OpenAI Whisper API for audio transcription and GPT-4o mini for emotion/tone analysis.

## ğŸš€ Features

- **Audio Transcription**: Convert audio files to text using OpenAI Whisper API
- **Emotion & Tone Analysis**: Analyze transcribed text for emotion and tone using GPT-4o mini
- **File Upload**: Secure file upload with validation and size limits
- **Modular Architecture**: Clean separation of routes, controllers, and services
- **Error Handling**: Comprehensive error handling and logging
- **Development Logging**: Morgan logging in development mode
- **Environment Configuration**: Secure environment variable management

## ğŸ“ Project Structure

```
â”œâ”€â”€ server.js                 # Main server file
â”œâ”€â”€ package.json             # Dependencies and scripts
â”œâ”€â”€ .env                     # Environment variables (not in git)
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ uploads/                # Temporary audio file storage
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ transcribeRoutes.js # API routes for transcription
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ transcribeController.js # Business logic handlers
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ whisperService.js   # OpenAI Whisper API integration
â”‚   â””â”€â”€ emotionAnalysisService.js # GPT-4o mini emotion analysis
â””â”€â”€ utils/
    â””â”€â”€ helpers.js          # Utility functions
```

## ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd voice-notes-ai-backend
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   ```
   Then edit `.env` and add your OpenAI API key:
   ```
   OPENAI_API_KEY=your_actual_openai_api_key_here
   ```

4. **Start the server**
   ```bash
   # Development mode (with logging)
   npm run dev
   
   # Production mode
   npm start
   ```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | Your OpenAI API key | Required |
| `PORT` | Server port | 3000 |
| `NODE_ENV` | Environment mode | development |

### File Upload Limits

- **Maximum file size**: 25MB
- **Supported formats**: MP3, WAV, M4A, OGG
- **Storage**: Temporary files in `uploads/` directory

## ğŸ“¡ API Endpoints

### Health Check
```
GET /health
```
Returns server status and uptime information.

### Audio Transcription with Emotion Analysis
```
POST /api/transcribe/audio
```
Upload an audio file for transcription and emotion/tone analysis.

**Request:**
- Method: `POST`
- Content-Type: `multipart/form-data`
- Body: `audio` file field

**Response:**
```json
{
  "success": true,
  "message": "Audio transcribed and analyzed successfully",
  "data": {
    "transcription": "The transcribed text...",
    "language": "en",
    "duration": 10.5,
    "originalFilename": "audio.mp3",
    "emotion": "excited",
    "tone": "enthusiastic",
    "emotionReason": "Uses positive language and exclamation marks indicating high energy"
  }
}
```

## ğŸ§ª Testing

You can test the API using tools like Postman or curl:

```bash
# Test health endpoint
curl http://localhost:3000/health

# Test transcription with emotion analysis endpoint
curl -X POST \
  -F "audio=@path/to/your/audio.mp3" \
  http://localhost:3000/api/transcribe/audio
```

## ğŸ” Error Handling

The API provides detailed error responses for various scenarios:

- **400**: Invalid file type, file too large, missing file
- **401**: Invalid OpenAI API key
- **429**: Rate limit exceeded
- **500**: Server errors, API errors

## ğŸ›¡ï¸ Security Features

- File type validation
- File size limits
- Temporary file cleanup
- Environment variable protection
- Input sanitization

## ğŸ“ Logging

In development mode, the server logs:
- HTTP requests (Morgan)
- File processing status
- API calls to OpenAI Whisper
- Emotion analysis results
- Error details

## ğŸ§  Emotion & Tone Analysis

The backend now includes intelligent emotion and tone analysis using GPT-4o mini:

### Supported Emotions:
- happy, sad, angry, fearful, surprised, neutral
- excited, frustrated, anxious, calm, confused

### Supported Tones:
- serious, sarcastic, casual, excited, polite
- aggressive, formal, friendly, nervous, confident

### Analysis Features:
- **Real-time Processing**: Analyzes transcription immediately
- **Fallback Handling**: Continues if emotion analysis fails
- **Detailed Reasoning**: Provides explanation for detected emotion/tone
- **Multi-language Support**: Works with various languages including Urdu

## ğŸš€ Deployment

1. Set `NODE_ENV=production` in your environment
2. Ensure all environment variables are configured
3. Use a process manager like PM2:
   ```bash
   npm install -g pm2
   pm2 start server.js --name "voice-notes-backend"
   ```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the ISC License.

## ğŸ†˜ Support

For issues and questions:
1. Check the error logs
2. Verify your OpenAI API key
3. Ensure file format is supported
4. Check file size limits
5. Verify emotion analysis is working 